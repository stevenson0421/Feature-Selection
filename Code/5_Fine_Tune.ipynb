{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNSW-NB15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82332, 45)\n",
      "(175341, 45)\n",
      "(82332, 39)\n",
      "(175341, 39)\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_raw = pd.read_csv('../Data/UNSW_NB15/train.csv')\n",
    "print(train_raw.shape)\n",
    "test_raw = pd.read_csv('../Data/UNSW_NB15/test.csv')\n",
    "print(test_raw.shape)\n",
    "# Seperate label and drop unnecessary features\n",
    "train_X = train_raw.drop(['id', 'attack_cat', 'label'], axis=1).select_dtypes(include='number')\n",
    "print(train_X.shape)\n",
    "train_Y = train_raw['label']\n",
    "test_X = test_raw.drop(['id', 'attack_cat', 'label'], axis=1).select_dtypes(include='number')\n",
    "print(test_X.shape)\n",
    "test_Y = test_raw['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data with min, max of training data\n",
    "test_X1 = (test_X - train_X.min(axis=0)) / (train_X.max(axis=0) - train_X.min(axis=0))\n",
    "train_X1 = (train_X - train_X.min(axis=0)) / (train_X.max(axis=0) - train_X.min(axis=0))\n",
    "\n",
    "test_X1[test_X1 < 0] = 0\n",
    "test_X1[test_X1 > 1] = 1\n",
    "\n",
    "train_X1.fillna(0, inplace=True)\n",
    "test_X1.fillna(0, inplace=True)\n",
    "\n",
    "del train_X, test_X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.980176 using {'n_estimators': 2000} with all features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ccis229c\\Desktop\\Feature-Selection\\Code\\5_Fine_Tune.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ccis229c/Desktop/Feature-Selection/Code/5_Fine_Tune.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# get the best result\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ccis229c/Desktop/Feature-Selection/Code/5_Fine_Tune.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest: \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m using \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m with all features\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (grid_result\u001b[39m.\u001b[39mbest_score_, grid_result\u001b[39m.\u001b[39mbest_params_))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ccis229c/Desktop/Feature-Selection/Code/5_Fine_Tune.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m grid_result \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39;49mfit(train_X1, train_Y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ccis229c/Desktop/Feature-Selection/Code/5_Fine_Tune.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# get the best result\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ccis229c/Desktop/Feature-Selection/Code/5_Fine_Tune.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest: \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m using \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m with selected features\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (grid_result\u001b[39m.\u001b[39mbest_score_, grid_result\u001b[39m.\u001b[39mbest_params_))\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    885\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    887\u001b[0m     )\n\u001b[0;32m    889\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 891\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    893\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    895\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1391\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    831\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    832\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m     )\n\u001b[1;32m--> 838\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    839\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m         clone(base_estimator),\n\u001b[0;32m    841\u001b[0m         X,\n\u001b[0;32m    842\u001b[0m         y,\n\u001b[0;32m    843\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    844\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    845\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    846\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    847\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    848\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    849\u001b[0m     )\n\u001b[0;32m    850\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    851\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    852\u001b[0m     )\n\u001b[0;32m    853\u001b[0m )\n\u001b[0;32m    855\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    857\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    860\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    679\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 680\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    682\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\daal4py\\sklearn\\_device_offload.py:88\u001b[0m, in \u001b[0;36msupport_usm_ndarray.<locals>.decorator.<locals>.wrapper_with_self\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper_with_self\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m wrapper_impl(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\daal4py\\sklearn\\_device_offload.py:74\u001b[0m, in \u001b[0;36msupport_usm_ndarray.<locals>.decorator.<locals>.wrapper_impl\u001b[1;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m usm_iface \u001b[39m=\u001b[39m _extract_usm_iface(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     73\u001b[0m q, hostargs, hostkwargs \u001b[39m=\u001b[39m _get_host_inputs(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 74\u001b[0m result \u001b[39m=\u001b[39m _run_on_device(func, q, obj, \u001b[39m*\u001b[39mhostargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhostkwargs)\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m usm_iface \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(result, \u001b[39m'\u001b[39m\u001b[39m__array_interface__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m _copy_to_usm(q, result)\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\daal4py\\sklearn\\_device_offload.py:65\u001b[0m, in \u001b[0;36m_run_on_device\u001b[1;34m(func, queue, obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[39mwith\u001b[39;00m sycl_context(\u001b[39m'\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m queue\u001b[39m.\u001b[39msycl_device\u001b[39m.\u001b[39mis_gpu \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     63\u001b[0m                           host_offload_on_fail\u001b[39m=\u001b[39mhost_offload):\n\u001b[0;32m     64\u001b[0m             \u001b[39mreturn\u001b[39;00m dispatch_by_obj(obj, func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch_by_obj(obj, func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\daal4py\\sklearn\\_device_offload.py:53\u001b[0m, in \u001b[0;36m_run_on_device.<locals>.dispatch_by_obj\u001b[1;34m(obj, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdispatch_by_obj\u001b[39m(obj, func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 53\u001b[0m         \u001b[39mreturn\u001b[39;00m func(obj, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\daal4py\\sklearn\\ensemble\\_forest.py:672\u001b[0m, in \u001b[0;36mRandomForestClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[39m@support_usm_ndarray\u001b[39m()\n\u001b[0;32m    646\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    647\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[39m    Build a forest of trees from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[39m    self : object\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 672\u001b[0m     \u001b[39mreturn\u001b[39;00m _fit_classifier(\u001b[39mself\u001b[39;49m, X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\daal4py\\sklearn\\ensemble\\_forest.py:346\u001b[0m, in \u001b[0;36m_fit_classifier\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    344\u001b[0m _patching_status\u001b[39m.\u001b[39mwrite_log()\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m _dal_ready:\n\u001b[1;32m--> 346\u001b[0m     _daal_fit_classifier(\u001b[39mself\u001b[39;49m, X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    348\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_estimators_\n\u001b[0;32m    350\u001b[0m     \u001b[39m# Decapsulate classes_ attributes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ccis229c\\Anaconda3\\envs\\myenv\\lib\\site-packages\\daal4py\\sklearn\\ensemble\\_forest.py:248\u001b[0m, in \u001b[0;36m_daal_fit_classifier\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_estimators_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[39m# compute\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m dfc_trainingResult \u001b[39m=\u001b[39m dfc_algorithm\u001b[39m.\u001b[39;49mcompute(X, y, sample_weight)\n\u001b[0;32m    250\u001b[0m \u001b[39m# get resulting model\u001b[39;00m\n\u001b[0;32m    251\u001b[0m model \u001b[39m=\u001b[39m dfc_trainingResult\u001b[39m.\u001b[39mmodel\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hyperparameters to search\n",
    "n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
    "\n",
    "grid = dict(n_estimators=n_estimators)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X1, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.980176 using {'max_features': 'auto'} with all features\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters to search\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "grid = dict(max_features=max_features)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=2000)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X1, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.980176 using {'max_depth': 60} with all features\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters to search\n",
    "max_features = grid_result.best_params_['max_features']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, 11)]\n",
    "\n",
    "grid = dict(max_depth=max_depth)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=2000, max_features=max_features)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X1, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.980176 using {'min_samples_leaf': 1, 'min_samples_split': 2} with all features\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters to search\n",
    "max_depth = grid_result.best_params_['max_depth']\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "grid = dict(min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=2000, max_features=max_features, max_depth=max_depth)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X1, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9198814650348711\n"
     ]
    }
   ],
   "source": [
    "min_samples_split = grid_result.best_params_['min_samples_split']\n",
    "min_samples_leaf = grid_result.best_params_['min_samples_leaf']\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, \n",
    "                                  n_estimators=2000, max_features=max_features, max_depth=max_depth, \n",
    "                                  min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "model_rf.fit(train_X1, train_Y)\n",
    "predict = model_rf.predict(test_X1)\n",
    "print(f1_score(test_Y, predict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sbytes', 'dbytes', 'sloss', 'dloss', 'dwin', 'tcprtt', 'ackdat', 'ct_dst_ltm', 'ct_srv_src', 'ct_src_dport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports']\n",
      "(82332, 24)\n",
      "(175341, 24)\n"
     ]
    }
   ],
   "source": [
    "# correlation based feature selection\n",
    "corr = train_X1.corr().abs()\n",
    "\n",
    "correlation_threshold = 0.8\n",
    "corr.values[np.tril_indices_from(corr.values)] = np.nan\n",
    "redundant = []\n",
    "for j in corr.columns:\n",
    "    for i in corr.index:\n",
    "        if corr.loc[i, j] > correlation_threshold:\n",
    "            redundant.append((i, j))\n",
    "\n",
    "train_corr = train_X1.copy()\n",
    "train_corr['Label'] = train_Y\n",
    "corr2 = train_corr.corr().abs()\n",
    "\n",
    "corr3 = corr2['Label'].iloc[:-1].copy()\n",
    "drop = []\n",
    "\n",
    "# drop features having lower correlation with label\n",
    "for i, j in redundant:\n",
    "    if corr3[i] > corr3[j]:\n",
    "        if j not in drop:\n",
    "            drop.append(j)\n",
    "    elif i not in drop:\n",
    "        drop.append(i)\n",
    "print(drop)\n",
    "\n",
    "del corr, train_corr, corr2, corr3\n",
    "\n",
    "train_X2 = train_X1.drop(drop, axis=1)\n",
    "print(train_X2.shape)\n",
    "test_X2 = test_X1.drop(drop, axis=1)\n",
    "print(test_X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.944857 using {'n_estimators': 2000} with all features\n",
      "Best: 0.944857 using {'max_features': 'auto'} with all features\n",
      "Best: 0.944896 using {'max_depth': 40} with all features\n"
     ]
    }
   ],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
    "grid = dict(n_estimators=n_estimators)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X2, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "n_estimators = grid_result.best_params_['n_estimators']\n",
    "max_features = ['auto', 'sqrt']\n",
    "grid = dict(max_features=max_features)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=n_estimators)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X2, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "max_features = grid_result.best_params_['max_features']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, 11)]\n",
    "\n",
    "grid = dict(max_depth=max_depth)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=n_estimators, max_features=max_features)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X2, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "max_depth = grid_result.best_params_['max_depth']\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "grid = dict(min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=n_estimators, max_features=max_features, max_depth=max_depth)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X2, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "min_samples_split = grid_result.best_params_['min_samples_split']\n",
    "min_samples_leaf = grid_result.best_params_['min_samples_leaf']\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, \n",
    "                                  n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, \n",
    "                                  min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "model_rf.fit(train_X2, train_Y)\n",
    "predict = model_rf.predict(test_X2)\n",
    "print(f1_score(test_Y, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.944966 using {'min_samples_leaf': 1, 'min_samples_split': 2} with all features\n",
      "0.9289842632331903\n"
     ]
    }
   ],
   "source": [
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "grid = dict(min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=2000, max_features='auto', max_depth=40)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X2, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "min_samples_split = grid_result.best_params_['min_samples_split']\n",
    "min_samples_leaf = grid_result.best_params_['min_samples_leaf']\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, \n",
    "                                  n_estimators=2000, max_features='auto', max_depth=40, \n",
    "                                  min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "model_rf.fit(train_X2, train_Y)\n",
    "predict = model_rf.predict(test_X2)\n",
    "print(f1_score(test_Y, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
    "grid = dict(n_estimators=n_estimators)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X2, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "n_estimators = grid_result.best_params_['n_estimators']\n",
    "max_features = ['auto', 'sqrt']\n",
    "grid = dict(max_features=max_features)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=n_estimators)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X2, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "max_features = grid_result.best_params_['max_features']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, 11)]\n",
    "\n",
    "grid = dict(max_depth=max_depth)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=n_estimators, max_features=max_features)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X2, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "max_depth = grid_result.best_params_['max_depth']\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "grid = dict(min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf)\n",
    "# grid search\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=n_estimators, max_features=max_features, max_depth=max_depth)\n",
    "cv = RepeatedStratifiedKFold(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=grid, scoring='f1', cv=cv, error_score=0)\n",
    "grid_result = grid_search.fit(train_X2, train_Y)\n",
    "# get the best result\n",
    "print(\"Best: %f using %s with all features\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "min_samples_split = grid_result.best_params_['min_samples_split']\n",
    "min_samples_leaf = grid_result.best_params_['min_samples_leaf']\n",
    "model_rf = RandomForestClassifier(random_state=0, n_jobs=-1, \n",
    "                                  n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, \n",
    "                                  min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "model_rf.fit(train_X2, train_Y)\n",
    "predict = model_rf.predict(test_X2)\n",
    "print(f1_score(test_Y, predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47bdff014318604fa210da353932788820ee8531c76f980c2092f0fcb79da61a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
